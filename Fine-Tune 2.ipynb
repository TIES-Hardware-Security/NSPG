{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the necessary modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://www.datacamp.com/tutorial/fine-tuning-llama-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] The specified module could not be found. Error loading \"c:\\Users\\ssm220008\\OneDrive - The University of Texas at Dallas\\Research Projects\\NSPG\\NSPG\\.venv\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      5\u001b[0m     AutoModelForCausalLM,\n\u001b[0;32m      6\u001b[0m     AutoTokenizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     logging,\n\u001b[0;32m     11\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ssm220008\\OneDrive - The University of Texas at Dallas\\Research Projects\\NSPG\\NSPG\\.venv\\Lib\\site-packages\\torch\\__init__.py:148\u001b[0m\n\u001b[0;32m    146\u001b[0m                 err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(ctypes\u001b[38;5;241m.\u001b[39mget_last_error())\n\u001b[0;32m    147\u001b[0m                 err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 148\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    150\u001b[0m     kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder, lib_name):\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found. Error loading \"c:\\Users\\ssm220008\\OneDrive - The University of Texas at Dallas\\Research Projects\\NSPG\\NSPG\\.venv\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model from Hugging Face hub\n",
    "base_model = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "# Fine-tuned model \n",
    "new_model = \"falcon-7b-Property Classification\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset, model, and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('json', data_files='./train_dataset', split = 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-bit quantization configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-bit quantization via QLoRA allows efficient finetuning of huge LLM models on consumer hardware while retaining high performance. This dramatically improves accessibility and usability for real-world applications.\n",
    "\n",
    "QLoRA quantizes a pre-trained language model to 4 bits and freezes the parameters. A small number of trainable Low-Rank Adapter layers are then added to the model.\n",
    "\n",
    "During fine-tuning, gradients are backpropagated through the frozen 4-bit quantized model into only the Low-Rank Adapter layers. So, the entire pretrained model remains fixed at 4 bits while only the adapters are updated. Also, the 4-bit quantization does not hurt model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Llama 2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_id = \"cuda:0\"\n",
    "device = torch.device(gpu_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=quant_config,\n",
    "    device_map= 'auto' # device\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEFT parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional fine-tuning of pre-trained language models (PLMs) requires updating all of the model's parameters, which is computationally expensive and requires massive amounts of data.\n",
    "\n",
    "\n",
    "\n",
    "Parameter-Efficient Fine-Tuning (PEFT) works by only updating a small subset of the model's parameters, making it much more efficient. Learn about parameters by reading the PEFT official documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If targeting all linear layers\n",
    "target_modules = ['q_proj','k_proj','v_proj']\n",
    "\n",
    "# Recent work as shown in the QLoRA paper by Dettmers et al. suggests that targeting all linear layers results in better adaptation quality.\n",
    "# 'o_proj','gate_proj','down_proj','up_proj' ,'lm_head'\n",
    "peft_params = LoraConfig(\n",
    "    lora_alpha=256,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules = target_modules,\n",
    "    r=256,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters\n",
    "Below is a list of hyperparameters that can be used to optimize the training process:\n",
    "\n",
    "* output_dir: The output directory is where the model predictions and checkpoints will be stored.\n",
    "* num_train_epochs: One training epoch.\n",
    "* fp16/bf16: Disable fp16/bf16 training.\n",
    "* per_device_train_batch_size: Batch size per GPU for training.\n",
    "* per_device_eval_batch_size: Batch size per GPU for evaluation.\n",
    "* gradient_accumulation_steps: This refers to the number of steps required to accumulate the gradients during the update process.\n",
    "* gradient_checkpointing: Enabling gradient checkpointing.\n",
    "* max_grad_norm: Gradient clipping.\n",
    "* learning_rate: Initial learning rate.\n",
    "* weight_decay: Weight decay is applied to all layers except bias/LayerNorm weights.\n",
    "* Optim: Model optimizer (AdamW optimizer).\n",
    "* lr_scheduler_type: Learning rate schedule.\n",
    "* max_steps: Number of training steps.\n",
    "* warmup_ratio: Ratio of steps for a linear warmup.\n",
    "* group_by_length: This can significantly improve performance and accelerate the training process.\n",
    "* save_steps: Save checkpoint every 25 update steps.\n",
    "* logging_steps: Log every 25 update steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = TrainingArguments(\n",
    "    output_dir=\"./Trained_models\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=5,\n",
    "    auto_find_batch_size=False,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=5,\n",
    "    save_steps=54486,\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fine-tuning\n",
    "\n",
    "Supervised fine-tuning (SFT) is a key step in reinforcement learning from human feedback (RLHF). The TRL library from HuggingFace provides an easy-to-use API to create SFT models and train them on your dataset with just a few lines of code. It comes with tools to train language models using reinforcement learning, starting with supervised fine-tuning, then reward modeling, and finally proximal policy optimization (PPO).\n",
    "\n",
    "We will provide SFT Trainer the model, dataset, Lora configuration, tokenizer, and training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_params,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=2048,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_params,\n",
    "    packing=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(new_model)\n",
    "trainer.tokenizer.save_pretrained(new_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
